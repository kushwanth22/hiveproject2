Project Flow:
1. Data ingestion and acquisition is done through Sqoop from RDBMS and into Linux file system
from Cloud or any other sources.
2. Merge the 2 dataset using hive and split the defaulters and non-defaulters into 2 data sets and
load into hdfs.
3. Create a hive table with header line count as 1.
4. Create one more fixed width hive table to load the fixed width states_fixed width data.
5. Create and load penality data into the hive table serialized with orc.
6. Export the Defaulters and Non Defaulters data into HDFS with comma delimiter, where non
defaulters data added with trailer data for consumer systems validation.
7. Perform dist copy of non defaulters data from Prod cluster to non prod clusters that will be used
for analytics purposes.
8. Run a shell script to pull the data from Cloud S3, validate, remove trailer data, move to HDFS
and archive the data for backup
9. Create a partitioned table and run a shell script to which has to generate the load command and
it has to load the data into the below insurance table automatically
10. Create a fixed width hive table to load the fixed width states_fixedwidth data using Regex Serde
11. Apply Data Governance - Redaction and Masking using Hive and Python
12. Data Provisioning to the Consumers into legacy Databases using Sqoop including Validation
13. Complex type ETL using Hive
14. Data movement & migration from DB to HBase using Sqoop
15. Data movement & migration from Hive to HBase using Storage Handler
Sqoop Direct Import to HBase

Data Consumers

Realtime Aggregation Queries
for Analytics & Reporting

Legacy RDBMS
Reports
Batch/Interactive
OLAP Reports
DISTCP from Prod Cluster to Non Prod Cluster
Non Prod
Hadoop Cluster

PROD CLUSTER (HADOOP)

16. Data Provisioning to the Consumers using Apache Phoenix to support Realtime aggregation and
Low Latency Query processing
17. Write queries to build cubes at different levels to aggregate the data in realtime to populate in
the report such as average age, sum of bill amount, average bill amount etc.,
Login to VMWare –
Prerequisites:
Start Hadoop, history server, hive in mr mode, mysql service, hive metastore, hbase and phoenix

Go to:
Player -> Manage -> Virtual Machine Settings -> change the memory as 3072 MB or upto 6208 MB and
cpu core to 2

Extraction of Source code & data:
Download into /home/hduser and Untar the data provided in creditcard_insurance.tar.gz
cd ~
tar xvzf creditcard_insurance.tar.gz

DataSet:
All extracted scripts, datasets and other files will be in the given below location.
/home/hduser/creditcard_insurance
Start the Following services:
Hadoop:
start-all.sh
mr-jobhistory-daemon.sh start historyserver
Login, start mysql service and exit:
service mysqld start
sudo mysql -u root -p
password: root

Data preparation in the source systems
DB to Hadoop Data Ingestion:
Note: (This will be happening automatically in the source systems)
Insert into the mysql database - custmaster data in a table and the credit data into 2 tables of 2
timezones:
create database if not exists custdb;
use custdb;
drop table if exists credits_pst;
drop table if exists credits_cst;
drop table if exists custmaster;
create table if not exists credits_pst (id integer,lmt integer,sex integer,edu integer,marital integer,age
integer,pay integer,billamt integer,defaulter integer,issuerid1 integer,issuerid2 integer,tz varchar(3));
create table if not exists credits_cst (id integer,lmt integer,sex integer,edu integer,marital integer,age
integer,pay integer,billamt integer,defaulter integer,issuerid1 integer,issuerid2 integer,tz varchar(3));
create table if not exists custmaster (id integer,fname varchar(100),lname varchar(100),ageval
integer,profession varchar(100));
source /home/hduser/creditcard_insurance/2_2_creditcard_defaulters_pst
source /home/hduser/creditcard_insurance/2_creditcard_defaulters_cst
source /home/hduser/creditcard_insurance/custmaster

Sqoop integration with Hive + ETL & ELT operations (Merge the 2 dataset using Hive and split the
defaulters and non-defaulters into 2 data sets and load into hdfs. )
Sqoop and Hive integration with more options:
Import the creditcard datasets of cst and pst timezones into hive table with the below given steps.
1. Create a database in hive as insure.
create database if not exists insure;
2. Sqoop import the data into hive table insure.credits_pst and insure.credits_cst with options
such as hive overwrite, number of mappers 2, where hive table created by sqoop with id as
bigint, billamt as float (this is do able with –map-column-hive option checked the
cookbooks/online/solutions online)

for first hive table creating from direct table option
sqoop import --connect jdbc:mysql://localhost/custdb --username root --password root \
--table credits_pst --target-dir '/user/hduser/credits_pst' \
--split-by id -m 2 --hive-overwrite --hive-import --create-hive-table --fields-terminated-by ',' \
--hive-table insure.credits_pst --map-column-hive id=bigint,billamt=float --direct

(or) for second hive table creation I am using the query option with boundary-query specified but anyway
it not makes much difference since its a simple query without any joins.so boundary query is simple here
sqoop import --connect jdbc:mysql://localhost/custdb --username root --password root \
--boundary-query 'select min(id),max(id) from credits_cst' \
--query 'select * from credits_cst where $CONDITIONS' --target-dir '/user/hduser/credits_cst' \
--split-by id -m 2 --hive-overwrite --hive-import --create-hive-table --fields-terminated-by ',' \
--hive-table insure.credits_cst --map-column-hive id=bigint,billamt=float --direct

ETL & ELT using Hive
3. Filter the cst and pst tables with the billamt > 0 and union all both dataset and load into a
single table called insure.cstpstreorder using create table as select (CTAS)

create table insure.cstpstreorder
row format delimited fields terminated by ','
lines terminated by '\n'
stored as textfile as select * from credits_cst where billamt > 0
union all
select * from credits_pst where billamt > 0;


4. Note: We can merge the step 2 and 3 and make the above 2 sqoop statements as 1 sqoop
statement to directly import data into the insure.cstpstreorder by writing –query with billamt>0
filter and union of both credits_pst and credits_cst at the MYSQL level itself finally convert the
insure.cstpstreorder into external table. If you have some time try it out..

drop table insure.cstpstreorder;
Now create hive stable by sqooping from mysql with joining the tables and import the data directly to hive
managed table then alter table to external table
--direct uses native drivers of mysql while importing data that improves performance.
here default 4 mappers executes since we are not specifying -m so it required split-by
STEP:1

sqoop import --connect jdbc:mysql://localhost/custdb --username root --password root \
--query "select * from credits_cst where billamt > 0 and \$CONDITIONS
         union all
         select * from credits_pst where billamt > 0 and \$CONDITIONS" \
--target-dir '/user/hduser/cstpstreorder' \
--split-by id --as-textfile --hive-overwrite --hive-import --create-hive-table \
--hive-table insure.cstpstreorder \
--map-column-hive id=bigint,billamt=float --direct;
STEP:2
ALTER table insure.cstpstreorder SET TBLPROPERTIES('EXTERNAL'='TRUE');

5. Create another external table insure.cstpstpenality in orc file format (show create table
insure.cstpstreorder) to get the ddl of the above table and alter as per the below columns for
faster table creation and populate with the below ETL transformations applied in the above
table cstpstreorder as given below

CREATE external TABLE `insure.cstpstpenality`(
`id` bigint,
`issuerid1` int,
`issuerid2` int,
`lmt` int,
`newlmt` int,
`sex` int,
`edu` int,
`marital` int,
`pay` int,
`billamt` float,
`newbillamt` float,
`defaulter` int)
stored as orc
LOCATION
'/user/hduser/cstpstpenality';

    a. id,issuerid1,issuerid2,lmt,case defaulter when 1 then lmt-(lmt*0.04) else lmt end as
    newlmt ,sex,edu,marital,pay,billamt,case defaulter when 1 then billamt+(billamt*0.02) else
    billamt end as newbillamt, defaulter;

    insert overwrite table cstpstpenality select id,issuerid1,issuerid2,lmt,
    case defaulter when 1 then lmt-(lmt*0.04) else lmt end as newlmt,
    sex,edu,marital,pay,billamt,
    case defaulter when 1 then billamt+(billamt*0.02) else billamt end as newbillamt,
    defaulter from cstpstreorder;

   -Data Provisioning using Hive, Sqoop and DistCp
     b. Export and overwrite the above data into /user/hduser/defaultersout/ we will be using
     this defaultersout data in a later point of time and /user/hduser/nondefaultersout/
     locations with defaulter=1 and defaulter=0 respectively using ‘,’ delimiter. We will be
     sending this nondefaultersout data to external systems using Distcp/SFTP.

     insert overwrite directory '/user/hduser/defaultersout/'
     row format delimited fields terminated by ','
     lines terminated by '\n'
     select * from cstpstpenality where defaulter=1;

     insert overwrite directory '/user/hduser/nondefaultersout/'
     row format delimited fields terminated by ', '
     select concat(id, ', ', issuerid1, ', ', issuerid2, ', ', lmt, ', ', newlmt, ', ', sex, ', ', edu, ', ', marital,
     ',', pay, ', ', billamt, ', ', newbillamt, ', ', defaulter ) as cnt from insure.cstpstpenality
     where defaulter=0
     union
     select concat('Trl|',count(1)) as cnt from (select * from insure.cstpstpenality
     where defaulter=0) as tmp;

-Data Provisioning to the Consumers using DistCP
    Copy the data non defaulters data from one cluster (Prod) to another cluster (Non Prod) for analytics
    purpose.

    hadoop distcp -overwrite hdfs://localhost:54310/user/hduser/nondefaultersout/
    hdfs://localhost:54310/tmp/promocustomers

-Data preparation in the source systems
-File system data source ingestion from Cloud using Linux Shell Script
    bash creditcard_insurance/sfm_insuredata.sh https://s3.amazonaws.com/in.inceptez.bucket1/insurance_project/insuranceinfo.csv

    Ensure data is imported from cloud to hdfs and get the date and take a note of the timestamp in the
    file
    hadoop fs -ls /user/hduser/insurance_clouddata

ETL & ELT using Hive
Create a hive table with header line count as 1 and load the insurance dataset.
drop table if exists insurance;

CREATE TABLE insurance (IssuerId1 int,IssuerId2 int,BusinessYear int,StateCode string,SourceName
string,NetworkName string,NetworkURL string,RowNumber int,MarketCoverage string,DentalOnlyPlan
string)
row format delimited fields terminated by ','
TBLPROPERTIES ("skip.header.line.count"="1");

load data inpath '/user/hduser/insurance_clouddata' into table insurance;

(Or)
-Create a partitioned table and load manually or by modifying the below script which has to generate
the load command and it has to load the data into the below insurance table automatically.

use insure;

CREATE TABLE insurancepartition (IssuerId1 int,IssuerId2 int,BusinessYear int,StateCode string,SourceName
string,NetworkName string,NetworkURL string,RowNumber int,MarketCoverage string,DentalOnlyPlan
string)
Partitioned by (datadt date,hr int)
row format delimited fields terminated by ','
TBLPROPERTIES ("skip.header.line.count"="1");


bash creditcard_insurance/sfm_insuredata.sh https://s3.amazonaws.com/in.inceptez.bucket1/insurance_project/insuranceinfo.csv

then
bash /home/hduser/creditcard_insurance/hivepart.sh /user/hduser/insurance_clouddata insure.insurancepartition creditcard_insurance

-Delete the invalid data with null issuerid1 and issuerid2 using insert select query

insert overwrite table insurance
select * from insurance where issuerid1 is not null and issuerid2 is not null;

insert overwrite table insurance partition(datadt,hr)
select * from insurance where issuerid1 is not null and issuerid2 is not null;

-Create a fixed width hive table to load the fixed width states_fixedwidth data using Regex Serde
CREATE EXTERNAL TABLE insure.state_master (statecd STRING, statedesc STRING)
ROW FORMAT SERDE 'org.apache.hadoop.hive.contrib.serde2.RegexSerDe'
WITH SERDEPROPERTIES ("input.regex"="(.{2})(.*)")
LOCATION '/user/hduser/states';

load data local inpath '/home/hduser/creditcard_insurance/states_fixedwidth'
overwrite into table insure.state_master;

Create a managed table on top of the hive output defaulter’s dataset created above.

use insure;
CREATE TABLE defaulters (id int,IssuerId1 int,IssuerId2 int,lmt int,newlmt double,sex int,edu int,marital
int,pay int,billamt int,newbillamt float,defaulter int)
row format delimited fields terminated by ','
LOCATION '/user/hduser/defaultersout';

Create a final managed table (later convert to external table) in orc with snappy compression and load
the above 2 tables joined by applying different functions as given below .
This table should not allow duplicates when it is empty or if not using overwrite option.

use insure;
CREATE TABLE insurancestg (IssuerId int,BusinessYear int,StateCode string,statedesc string,SourceName
string,NetworkName string,NetworkURL string,RowNumber int,MarketCoverage string,DentalOnlyPlan
string,id int,lmt int,newlmt int,reduced_lmt int,sex varchar(6),grade varchar(20),marital int,pay
int,billamt int,newbillamt float,penality float,defaulter int)
row format delimited fields terminated by ','
LOCATION '/user/hduser/insurancestg';

insert overwrite table insurancestg select concat(i.IssuerId1,i.IssuerId2) as
issuerid,i.businessyear,i.statecode,s.statedesc as
statedesc,i.sourcename,i.networkname,i.networkurl,i.rownumber,i.marketcoverage,i.dentalonlyplan,
d.id,d.lmt,d.newlmt,d.newlmt-d.lmt as reduced_lmt,case when d.sex=1 then 'male' else 'female' end as
sex ,case when d.edu=1 then 'lower grade' when d.edu=2 then 'lower middle grade' when d.edu=3 then
'middle grade' when d.edu=4 then 'higher grade' when d.edu=5 then 'doctrate grade' end as grade
,d.marital,d.pay,d.billamt,d.newbillamt,d.newbillamt-d.billamt as penality,d.defaulter
from insurance i inner join defaulters d
on (i.IssuerId1=d.IssuerId1 and i.IssuerId2=d.IssuerId2)
inner join state_master s
on (i.statecode=s.statecd);

hadoop fs -rm -r -f /user/hduser/insuranceorc;
drop table if exists insuranceorc;

CREATE TABLE insuranceorc (IssuerId int,BusinessYear int,StateCode string,statedesc string,SourceName
string,NetworkName string,NetworkURL string,RowNumber int,MarketCoverage string,DentalOnlyPlan
string,id int,lmt int,newlmt int,reduced_lmt int,sex varchar(6),grade varchar(20),marital int,pay
int,billamt int,newbillamt float,penality int,defaulter int)
row format delimited fields terminated by ','
stored as orc
LOCATION '/user/hduser/insuranceorc'
TBLPROPERTIES ("immutable"="true","orc.compress"="SNAPPY");

Insert into insuranceorc select * from insurancestg where issuerid is not null;

Retry running the above same insert query once again and see what happens??
If you get the below error then drop the above table and recreate and insert.

FAILED: SemanticException [Error 10256]: Inserting into a non-empty immutable table is not allowed
insuranceorc
Convert the above table from managed to external, usually we use the below statement if we can’t
create external table in the initial stage itself for example sqoop import hive table can’t be created as
external initially.
ans:
alter table insuranceorc SET TBLPROPERTIES('EXTERNAL'='TRUE');

-write common table expression queries in hive
ans:
with T1 as ( select max(penality) as penalitymale from insuranceorc where sex='male'),
T2 as ( select max(penality) as penalityfemale from insuranceorc where sex='female')
select penalitymale,penalityfemale
from T1 inner join T2
ON 1=1;

Data Governance - Redaction and Masking using Hive and Python
create view in hive to restrict few columns, store queries, and apply some masking on sensitive
columns using either query or by using the mask_insure.py function given in the project document.

Data Governance - Redaction and Masking using Hive and Python
create view in hive to restrict few columns, store queries, and apply some masking on sensitive
columns using either query or by using the mask_insure.py function given in the project document.
ans:
use insure;
drop view if exists middlegradeview;
create view middlegradeview as
select issuerid,businessyear,statedesc,sourcename, sex,grade,marital,newbillamt,defaulter
,translate(translate(translate(translate(translate(networkurl,'a','x'),'b','y'),'c','z'),'s','r'),'.com','.aaa') as
maskednetworkurl
from insuranceorc
where grade='middle grade'
and issuerid is not null;

(or)

create view middlegradeview2 as
select transform(issuerid,businessyear,statedesc,sourcename, defaulter ,networkurl) using 'python /home/hduser/creditcard_insurance/mask_insure.py'
as (issuerid,businessyear,statedesc,sourcename, defaulter ,maskednetworkurl)
from insuranceorc
where grade='middle grade'
and issuerid is not null;
select * from middlegradeview;

create view middlegradeview0 as
select issuerid,businessyear,statedesc,sourcename, defaulter ,networkurl
from insuranceorc where grade='middle grade'
and issuerid is not null;
select * from middlegradeview;

Export the above view data into hdfs location /user/hduser/defaulterinfo with the pipe ‘|’ delimiter
the following columns
issuerid,businessyear,statedesc,sourcename,maskednetworkurl,sex,grade,marital,newbillamt,defaulter
ans:
insert overwrite directory '/user/hduser/defaulterinfo/'
row format delimited fields terminated by '|'
select * from insure.middlegradeview
where defaulter=1;

Data Provisioning to the Consumers into legacy Databases using Sqoop
including Validation
Data export using Sqoop into DB
Export the above data into mysql using sqoop

ans:
mysql -u root -p
password: root
use custdb;
drop table if exists middlegrade;

create table middlegrade (issuerid integer,businessyear integer,maskedstatedesc
varchar(200),maskedsourcename varchar(100), defaulter varchar(20),maskednetworkurl varchar(200));
quit;

1. Export the masked data into mysql using sqoop export as per the above table structure.
2. Validate the export is properly happened using --validate option in sqoop
ans:
sqoop export --connect jdbc:mysql://localhost/custdb --username root --password root \
--table middlegrade --export-dir '/user/hduser/defaulterinfo' --fields-terminated-by '|' --validate;

Complex type ETL using Hive
Create a Complex type table to understand how to group the like issuers in a single row using array,
struct and map.
ans:
Drop table if exists insuranceorc_collection ;
Create table insuranceorc_collection
row format delimited collection items terminated by ', '
stored as orcfile
location '/user/hduser/insuranceorc_collection/'
as select issuerid,named_struct('marital',marital,'sex',sex,'grade',grade) as
personalinfo,collect_set(networkname) as networkname, collect_set(networkurl) networkurl
from insuranceorc group by issuerid,named_struct('marital',marital,'sex',sex,'grade',grade);

alter table insuranceorc_collection SET TBLPROPERTIES('EXTERNAL'='TRUE');

Select only the issuerid,grade,second networkname,second networkurl where we have more than 1
networkname and networkurl accessed by the customers.
ans:
select issuerid,personalinfo.grade,networkname[1],networkurl[1]
from insuranceorc_collection where size(networkname) > 1 and size(networkurl) > 1;

Data movement & migration from DB to HBase using Sqoop
rdbms <-> sqoop tool <-> hbase <-> hive
Hive HBase handler data load
Join insurance and credit card data and load into hbase table created with 2 column families credit and
insurance.
ans:
hbase shell
create 'custmaster', 'customer'
sqoop import --connect jdbc:mysql://localhost/custdb --username root --password root --table \
custmaster --hbase-create-table --hbase-table custmaster --column-family customer --hbase-row-key id \
-m 1 -validate &> /tmp/sqoop.log;

1. Import using sqoop from db into hbase custmaster data.
ans:
sqoop import --connect jdbc:mysql://inceptez/custdb --username root --password root --table
custmaster --hbase-create-table --hbase-table custmaster --column-family customer --hbase-row-key id -
m 1 -validate &> /tmp/sqoop.log

2. Validate the sqoop import is properly happened using --validate option in sqoop and create a
_SUCCESS file if the validation is successful in /home/hduser/creditcard_insurance location.
ans:
if [ `cat /tmp/sqoop.log | grep "Data successfully validated" | wc -l` -eq 1 ]
then
touch /home/hduser/creditcard_insurance/_HBASE_SUCCESS
fi

Data movement & migration from Hive to HBase using Storage
Handler

sourcefiles/tables  -> hive insert select -> hbase

Create a hbase handler table in hive using hbase storage handler referring to insurancehive table that
will be automatically created in hbase with insurance and credit card column families when we create
the below hive table.
ans:
Use insure;
drop table if exists insurancehive;
CREATE TABLE insurancehive (idkey int, issuerid int,id int,businessyear int,statedesc string,networkurl
string,pay int,defaulter string,billamt int,newbillamt float,penality float)
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler' WITH SERDEPROPERTIES
("hbase.columns.mapping" =
":key,insurance:issuerid,insurance:id,insurance:businessyear,insurance:statedesc,insurance:networkurl,
creditcard:pay,creditcard:defaulter,creditcard:billamt,creditcard:newbillamt,creditcard:penality")

TBLPROPERTIES ("hbase.table.name" = "insurancehive",
"hbase.mapred.output.outputtable"="insurancehive");

Insert incremental rowkey as the row_key into HBASE.
ans:
insert into table insurancehive select row_number() over() as
idkey,issuerid,id,businessyear,statedesc,networkurl,pay,defaulter,billamt,newbillamt,penality
from insuranceorc where issuerid is not null;

Data Provisioning to the Consumers using Apache Phoenix to support
Realtime aggregation and Low Latency Query processing
Create a phoenix table view on the above hbase table and analyze profession based total
payment and average payment.
ans:
sqlline.py localhost
!set maxwidth 1000
drop view if exists "insurancehive";
create view "insurancehive" (idkey varchar(100) primary key,"insurance"."issuerid"
varchar,"insurance"."id" varchar,"insurance"."businessyear" varchar,"insurance"."statedesc"
varchar,"insurance"."networkurl" varchar,"creditcard"."pay" varchar,"creditcard"."defaulter"
varchar,"creditcard"."billamt" varchar,"creditcard"."newbillamt" varchar,"creditcard"."penality"
varchar);
drop view if exists "custmaster";
create view "custmaster" (id varchar primary key,"customer"."fname"
varchar,"customer"."lname" varchar,"customer"."profession" varchar,"customer"."ageval" varchar);